services:
  llm-defender-validator-debug-mode:
    restart: unless-stopped
    user: llm-defender-validator-user
    ports:
      - "6000:6000"
    build:
      context: .
      dockerfile: validator.Dockerfile
    command: /bin/bash -c "source /llm-defender-subnet/.venv/bin/activate && python3 /llm-defender-subnet/llm_defender/neurons/validator.py --netuid ${NETUID} --subtensor.chain_endpoint ${SUBTENSOR_CHAIN_ENDPOINT} --wallet.name ${VALIDATOR_WALLET} --wallet.hotkey ${VALIDATOR_HOTKEY} --log_level ${LOG_LEVEL} --debug_mode"
    volumes:
      - llm-defender-subnet:/home/llm-defender-validator-user/.llm-defender-subnet
      - ${HOME}/.bittensor:/home/llm-defender-validator-user/.bittensor
  
  llm-defender-validator-remote-vllm:
    restart: unless-stopped
    user: llm-defender-validator-user
    ports:
      - "6000:6000"
    build:
      context: .
      dockerfile: validator.Dockerfile
    command: /bin/bash -c "source /llm-defender-subnet/.venv/bin/activate && python3 /llm-defender-subnet/llm_defender/neurons/validator.py --netuid ${NETUID} --subtensor.chain_endpoint ${SUBTENSOR_CHAIN_ENDPOINT} --wallet.name ${VALIDATOR_WALLET} --wallet.hotkey ${VALIDATOR_HOTKEY} --log_level ${LOG_LEVEL}"
    volumes:
      - llm-defender-subnet:/home/llm-defender-validator-user/.llm-defender-subnet
      - ${HOME}/.bittensor:/home/llm-defender-validator-user/.bittensor

  llm-defender-validator:
    depends_on: 
      - prompt-generation-api
    restart: unless-stopped
    user: llm-defender-validator-user
    ports:
      - "6000:6000"
    build:
      context: .
      dockerfile: validator.Dockerfile
    command: /bin/bash -c "source /llm-defender-subnet/.venv/bin/activate && python3 /llm-defender-subnet/llm_defender/neurons/validator.py --netuid ${NETUID} --subtensor.chain_endpoint ${SUBTENSOR_CHAIN_ENDPOINT} --wallet.name ${VALIDATOR_WALLET} --wallet.hotkey ${VALIDATOR_HOTKEY} --log_level ${LOG_LEVEL}"
    volumes:
      - llm-defender-subnet:/home/llm-defender-validator-user/.llm-defender-subnet
      - ${HOME}/.bittensor:/home/llm-defender-validator-user/.bittensor
  prompt-generation-api:
    restart: unless-stopped
    image: vllm/vllm-openai:v0.5.0
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    ipc: host
    command: [
      "--model", "TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ",
      "--revision", "gptq-8bit-128g-actorder_True",
      "--tensor-parallel-size", "${TENSOR_PARALLEL_SIZE}"
    ]
    volumes:
      - ${HOME}.cache/huggingface:/root/.cache/huggingface
  
  llm-defender-api-latest:
    restart: unless-stopped
    pull_policy: always
    image: ghcr.io/synapsec-ai/llm-defender-api:latest
    command: /bin/bash -c "source /llm-defender-subnet/.venv/bin/activate && python3 /llm-defender-subnet/llm_defender/subnet_api/main.py"
    ports:
      - "8080:8080"
    volumes:
      - ${HOME}/.bittensor:/.bittensor
    env_file:
      - .api-env
  llm-defender-api:
    restart: unless-stopped
    pull_policy: always
    image: ghcr.io/synapsec-ai/llm-defender-api:v0.8.2rc1
    command: /bin/bash -c "source /llm-defender-subnet/.venv/bin/activate && python3 /llm-defender-subnet/llm_defender/subnet_api/main.py"
    ports:
      - "8080:8080"
    volumes:
      - ${HOME}/.bittensor:/.bittensor
    env_file:
      - .api-env

volumes:
  llm-defender-subnet: